{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahithi530/Sahithi_INFO5731_Fall2024/blob/main/Tummala_Sahithi_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "outputId": "35bcaac4-d3c7-4c16-cf65-7a6a5c6a2f6e"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(confusion_matrix(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], test_predictions))\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[30], line 29\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m():\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Load your data\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Initialize vectorizer\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # Load your data with explicit delimiter handling\n",
        "    try:\n",
        "        train_data = pd.read_csv(\n",
        "            '/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt',\n",
        "            sep=\"\\t\",  # Adjust based on your file's delimiter\n",
        "            names=[\"text\", \"label\"],  # Provide column names if the file has no header\n",
        "            on_bad_lines='skip'  # Skip problematic lines\n",
        "        )\n",
        "        test_data = pd.read_csv(\n",
        "            '/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt',\n",
        "            sep=\"\\t\",  # Adjust based on your file's delimiter\n",
        "            names=[\"text\", \"label\"],  # Provide column names\n",
        "            on_bad_lines='skip'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Error loading files:\", e)\n",
        "        return\n",
        "\n",
        "    # Verify data loaded correctly\n",
        "    print(\"\\nTraining Data Sample:\")\n",
        "    print(train_data.head())\n",
        "    print(\"\\nTest Data Sample:\")\n",
        "    print(test_data.head())\n",
        "\n",
        "    # Initialize vectorizer\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "    # Split training data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train_data['text'], train_data['label'], test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Vectorize the text data\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_val_vec = vectorizer.transform(X_val)\n",
        "    X_test_vec = vectorizer.transform(test_data['text'])\n",
        "\n",
        "    # Initialize the classifier\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = clf.predict(X_val_vec)\n",
        "    val_metrics = evaluate_metrics(y_val, val_predictions)\n",
        "    for metric, value in val_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = clf.predict(X_test_vec)\n",
        "    test_metrics = evaluate_metrics(test_data['label'], test_predictions)\n",
        "    for metric, value in test_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(test_data['label'], test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WruBJ7skq7Er",
        "outputId": "5adc537a-8c66-4dbd-d5e5-725c9dbb1047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Training final model with reduced features...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.73      0.75       671\n",
            "           1       0.76      0.80      0.78       713\n",
            "\n",
            "    accuracy                           0.76      1384\n",
            "   macro avg       0.77      0.76      0.76      1384\n",
            "weighted avg       0.76      0.76      0.76      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.72      0.73       912\n",
            "           1       0.73      0.76      0.74       909\n",
            "\n",
            "    accuracy                           0.74      1821\n",
            "   macro avg       0.74      0.74      0.74      1821\n",
            "weighted avg       0.74      0.74      0.74      1821\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate_optimized():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Vectorizer with reduced features\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(train_data['text']).toarray()\n",
        "    X_test_vec = vectorizer.transform(test_data['text']).toarray()\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"Training final model with reduced features...\")\n",
        "    clf = SVC(kernel='linear', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = clf.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = clf.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_optimized()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmbdceF7q7Er",
        "outputId": "baf2c2ab-aa66-42f7-9906-3cc3f3d2ea18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Vectorizing text data...\n",
            "Training KNN model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.98      0.66       671\n",
            "           1       0.79      0.08      0.14       713\n",
            "\n",
            "    accuracy                           0.51      1384\n",
            "   macro avg       0.65      0.53      0.40      1384\n",
            "weighted avg       0.65      0.51      0.39      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.96      0.67       912\n",
            "           1       0.69      0.09      0.16       909\n",
            "\n",
            "    accuracy                           0.53      1821\n",
            "   macro avg       0.60      0.52      0.41      1821\n",
            "weighted avg       0.60      0.53      0.41      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[876  36]\n",
            " [828  81]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the text file data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in lines:\n",
        "        label = int(line[0])\n",
        "        text = line[2:].strip()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "def train_and_evaluate_knn():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Vectorizer with reduced features\n",
        "    print(\"Vectorizing text data...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(train_data['text']).toarray()\n",
        "    X_test_vec = vectorizer.transform(test_data['text']).toarray()\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)  # Set k to 5 (can be adjusted)\n",
        "\n",
        "    print(\"Training KNN model...\")\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = knn.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = knn.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_knn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tv5Pb4Fq7Er",
        "outputId": "31fe82fb-30b4-4261-b525-087f98b2eade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Vectorizing text data...\n",
            "Training Decision Tree model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.28      0.40       671\n",
            "           1       0.56      0.86      0.68       713\n",
            "\n",
            "    accuracy                           0.58      1384\n",
            "   macro avg       0.61      0.57      0.54      1384\n",
            "weighted avg       0.61      0.58      0.54      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.31      0.42       912\n",
            "           1       0.55      0.84      0.66       909\n",
            "\n",
            "    accuracy                           0.57      1821\n",
            "   macro avg       0.60      0.57      0.54      1821\n",
            "weighted avg       0.60      0.57      0.54      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[279 633]\n",
            " [145 764]]\n",
            "\n",
            "Decision Tree Rules:\n",
            "|--- too <= 0.20\n",
            "|   |--- and <= 0.08\n",
            "|   |   |--- bad <= 0.09\n",
            "|   |   |   |--- only <= 0.23\n",
            "|   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |   |--- only >  0.23\n",
            "|   |   |   |   |--- truncated branch of depth 6\n",
            "|   |   |--- bad >  0.09\n",
            "|   |   |   |--- not <= 0.29\n",
            "|   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |   |--- not >  0.29\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |--- and >  0.08\n",
            "|   |   |--- bad <= 0.21\n",
            "|   |   |   |--- more <= 0.22\n",
            "|   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |   |--- more >  0.22\n",
            "|   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |--- bad >  0.21\n",
            "|   |   |   |--- and <= 0.21\n",
            "|   |   |   |   |--- truncated branch of depth 2\n",
            "|   |   |   |--- and >  0.21\n",
            "|   |   |   |   |--- truncated branch of depth 2\n",
            "|--- too >  0.20\n",
            "|   |--- not <= 0.44\n",
            "|   |   |--- picture <= 0.14\n",
            "|   |   |   |--- time <= 0.27\n",
            "|   |   |   |   |--- truncated branch of depth 7\n",
            "|   |   |   |--- time >  0.27\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |--- picture >  0.14\n",
            "|   |   |   |--- class: 1\n",
            "|   |--- not >  0.44\n",
            "|   |   |--- class: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the text file data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in lines:\n",
        "        label = int(line[0])\n",
        "        text = line[2:].strip()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "def train_and_evaluate_decision_tree():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Vectorizer with reduced features\n",
        "    print(\"Vectorizing text data...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(train_data['text']).toarray()\n",
        "    X_test_vec = vectorizer.transform(test_data['text']).toarray()\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the Decision Tree Classifier\n",
        "    dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=10, random_state=42)\n",
        "\n",
        "    print(\"Training Decision Tree model...\")\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = dt.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = dt.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "    # Display the decision tree rules\n",
        "    print(\"\\nDecision Tree Rules:\")\n",
        "    tree_rules = export_text(dt, feature_names=vectorizer.get_feature_names_out(), max_depth=3)\n",
        "    print(tree_rules)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_decision_tree()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeIe6ypsq7Es",
        "outputId": "bce50af6-5a20-4c8f-83c0-fd36a29a10db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Vectorizing text data...\n",
            "Training Random Forest model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.38      0.50       671\n",
            "           1       0.60      0.89      0.72       713\n",
            "\n",
            "    accuracy                           0.64      1384\n",
            "   macro avg       0.68      0.63      0.61      1384\n",
            "weighted avg       0.68      0.64      0.61      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.41      0.53       912\n",
            "           1       0.59      0.86      0.70       909\n",
            "\n",
            "    accuracy                           0.64      1821\n",
            "   macro avg       0.67      0.64      0.62      1821\n",
            "weighted avg       0.67      0.64      0.62      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[371 541]\n",
            " [123 786]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the text file data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in lines:\n",
        "        label = int(line[0])\n",
        "        text = line[2:].strip()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "def train_and_evaluate_random_forest():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Vectorizer with reduced features\n",
        "    print(\"Vectorizing text data...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(train_data['text']).toarray()\n",
        "    X_test_vec = vectorizer.transform(test_data['text']).toarray()\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    print(\"Training Random Forest model...\")\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = rf.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = rf.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_random_forest()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0PfrExtq7Es",
        "outputId": "5015ce4e-a865-4ffb-f832-e7dfcbdd94fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Vectorizing text data...\n",
            "Training KNN model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.98      0.66       671\n",
            "           1       0.79      0.08      0.14       713\n",
            "\n",
            "    accuracy                           0.51      1384\n",
            "   macro avg       0.65      0.53      0.40      1384\n",
            "weighted avg       0.65      0.51      0.39      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.96      0.67       912\n",
            "           1       0.69      0.09      0.16       909\n",
            "\n",
            "    accuracy                           0.53      1821\n",
            "   macro avg       0.60      0.52      0.41      1821\n",
            "weighted avg       0.60      0.53      0.41      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[876  36]\n",
            " [828  81]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the text file data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in lines:\n",
        "        label = int(line[0])\n",
        "        text = line[2:].strip()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "def train_and_evaluate_knn():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Vectorizer with reduced features\n",
        "    print(\"Vectorizing text data...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(train_data['text']).toarray()\n",
        "    X_test_vec = vectorizer.transform(test_data['text']).toarray()\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    print(\"Training KNN model...\")\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = knn.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = knn.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_knn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vItCWieq7Es",
        "outputId": "630ed636-c535-4621-8db7-e71cf380657b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/funnysahithi/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Preprocessing text data...\n",
            "Training Word2Vec model...\n",
            "Training KNN model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.48      0.51       671\n",
            "           1       0.56      0.62      0.59       713\n",
            "\n",
            "    accuracy                           0.55      1384\n",
            "   macro avg       0.55      0.55      0.55      1384\n",
            "weighted avg       0.55      0.55      0.55      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.47      0.51       912\n",
            "           1       0.53      0.61      0.57       909\n",
            "\n",
            "    accuracy                           0.54      1821\n",
            "   macro avg       0.54      0.54      0.54      1821\n",
            "weighted avg       0.54      0.54      0.54      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[432 480]\n",
            " [357 552]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the text file data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for line in lines:\n",
        "        label = int(line[0])\n",
        "        text = line[2:].strip()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "def preprocess_text(texts):\n",
        "    \"\"\"\n",
        "    Tokenize the texts and preprocess by lowering the case and removing non-alphanumeric characters.\n",
        "    \"\"\"\n",
        "    return [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "def train_and_evaluate_word2vec_knn():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Preprocess the text data\n",
        "    print(\"Preprocessing text data...\")\n",
        "    train_texts = preprocess_text(train_data['text'])\n",
        "    test_texts = preprocess_text(test_data['text'])\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    word2vec_model = Word2Vec(sentences=train_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    word2vec_model.save(\"word2vec.model\")  # Save the trained Word2Vec model\n",
        "\n",
        "    # Convert texts to Word2Vec feature vectors (average word embeddings for each document)\n",
        "    def vectorize_text(text, model):\n",
        "        vectors = [model.wv[word] for word in text if word in model.wv]\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    X_train_vec = np.array([vectorize_text(text, word2vec_model) for text in train_texts])\n",
        "    X_test_vec = np.array([vectorize_text(text, word2vec_model) for text in test_texts])\n",
        "\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    print(\"Training KNN model...\")\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = knn.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = knn.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_word2vec_knn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5lzD-57q7Es",
        "outputId": "1e6b50b2-3bb2-4b3f-983f-22455c43add0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training data shape: (6920, 2)\n",
            "Test data shape: (1821, 2)\n",
            "Preprocessing text data...\n",
            "Loading BERT model...\n",
            "Generating BERT embeddings...\n",
            "Training KNN model...\n",
            "\n",
            "Validation Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.78       671\n",
            "           1       0.79      0.81      0.80       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.79      0.79      0.79      1384\n",
            "weighted avg       0.79      0.79      0.79      1384\n",
            "\n",
            "\n",
            "Test Set Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.73      0.75       912\n",
            "           1       0.74      0.79      0.77       909\n",
            "\n",
            "    accuracy                           0.76      1821\n",
            "   macro avg       0.76      0.76      0.76      1821\n",
            "weighted avg       0.76      0.76      0.76      1821\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[662 250]\n",
            " [187 722]]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_text(texts, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Tokenize the texts using BERT tokenizer, return input_ids and attention_masks.\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "    return encodings['input_ids'], encodings['attention_mask']\n",
        "\n",
        "def get_bert_embeddings(texts, model, tokenizer, batch_size=16):\n",
        "    \"\"\"\n",
        "    Generate BERT embeddings for a list of texts by batching and averaging the embeddings.\n",
        "    \"\"\"\n",
        "    input_ids, attention_mask = preprocess_text(texts, tokenizer)\n",
        "\n",
        "    # Create DataLoader for batching\n",
        "    dataset = TensorDataset(input_ids, attention_mask)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    embeddings = []\n",
        "    model.eval()  # Set model to evaluation mode to deactivate dropout layers\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids_batch, attention_mask_batch = batch\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "            # Take the mean of token embeddings in the last hidden state to represent the sentence\n",
        "            sentence_embedding = hidden_states.mean(dim=1).cpu().numpy()\n",
        "            embeddings.extend(sentence_embedding)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def train_and_evaluate_bert_knn():\n",
        "    print(\"Loading data...\")\n",
        "    train_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-train.txt')\n",
        "    test_data = load_data('/Users/funnysahithi/Downloads/exercise09_datacollection/stsa-test.txt')\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    # Preprocess the text data\n",
        "    print(\"Preprocessing text data...\")\n",
        "    train_texts = train_data['text'].tolist()  # Convert to list of strings\n",
        "    test_texts = test_data['text'].tolist()  # Convert to list of strings\n",
        "\n",
        "    # Load pre-trained BERT model\n",
        "    print(\"Loading BERT model...\")\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Generate embeddings using BERT\n",
        "    print(\"Generating BERT embeddings...\")\n",
        "    X_train_vec = get_bert_embeddings(train_texts, model, tokenizer)\n",
        "    X_test_vec = get_bert_embeddings(test_texts, model, tokenizer)\n",
        "\n",
        "    y_train = train_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_vec, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize the KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    print(\"Training KNN model...\")\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    val_predictions = knn.predict(X_val)\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    test_predictions = knn.predict(X_test_vec)\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, test_predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_bert_knn()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "outputId": "7b5d2f0b-5a91-473f-ee3d-7528a6865ea5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset (replace the path with your local path to the CSV file)\n",
        "file_path = '/Users/funnysahithi/Downloads/Amazon_Unlocked_Mobile.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Check if text is not a string (e.g., if it's NaN or a number)\n",
        "    if not isinstance(text, str):\n",
        "        return ''  # Return empty string for non-string values\n",
        "\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the text column, ensuring non-string values are handled\n",
        "data['cleaned_text'] = data['Reviews'].apply(preprocess_text)\n",
        "\n",
        "# Show the cleaned data\n",
        "print(data[['Reviews', 'cleaned_text']].head())\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to avoid memory issues\n",
        "X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 5  # Choose the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assign cluster labels to the data\n",
        "data['cluster'] = kmeans.labels_\n",
        "\n",
        "# Print the cluster assignments\n",
        "print(\"Cluster assignments:\")\n",
        "print(data[['Reviews', 'cleaned_text', 'cluster']].head(10))\n",
        "\n",
        "# Reduce dimensionality for visualization using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Plot the clustered data\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='rainbow', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"K-means Clustering Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze the clusters\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(data[data['cluster'] == i]['cleaned_text'].head(10))\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC9HQ3xZq7Et",
        "outputId": "be6c282e-8ccc-4604-c3bd-89c22387517e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/funnysahithi/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Data:\n",
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n",
            "\n",
            "Cleaned Data:\n",
            "                                             Reviews  \\\n",
            "0  I feel so LUCKY to have found this used (phone...   \n",
            "1  nice phone, nice up grade from my pantach revu...   \n",
            "2                                       Very pleased   \n",
            "3  It works good but it goes slow sometimes but i...   \n",
            "4  Great phone to replace my lost phone. The only...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  feel lucky found used phone us used hard phone...  \n",
            "1  nice phone nice grade pantach revue clean set ...  \n",
            "2                                            pleased  \n",
            "3     works good goes slow sometimes good phone love  \n",
            "4  great phone replace lost phone thing volume bu...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset (replace with your dataset path)\n",
        "file_path = '/Users/funnysahithi/Downloads/Amazon_Unlocked_Mobile.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(\"Original Data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Handle non-string values\n",
        "        return ''\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['cleaned_text'] = data['Reviews'].fillna(\"\").apply(preprocess_text)\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"\\nCleaned Data:\")\n",
        "print(data[['Reviews', 'cleaned_text']].head())\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=500)  # Reduce to 500 features for better performance\n",
        "X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Reduce dimensionality using PCA\n",
        "pca = PCA(n_components=50, random_state=42)  # Reduce to 50 dimensions for DBSCAN efficiency\n",
        "X_reduced = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "eps_value = 0.3  # Distance threshold\n",
        "min_samples_value = 10  # Minimum points per cluster\n",
        "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value, metric='euclidean')\n",
        "dbscan_labels = dbscan.fit_predict(X_reduced)\n",
        "\n",
        "# Assign cluster labels\n",
        "data['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# Visualize DBSCAN Clustering (2D PCA)\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_pca_2d = pca_2d.fit_transform(X_reduced)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter_dbscan = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter_dbscan)\n",
        "plt.title(\"DBSCAN Clustering Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Print clustering results\n",
        "print(\"\\nDBSCAN Clustering Results:\")\n",
        "unique_clusters = set(dbscan_labels)\n",
        "for cluster in unique_clusters:\n",
        "    print(f\"\\nCluster {cluster} ({'Noise' if cluster == -1 else 'Cluster'}):\")\n",
        "    print(data[data['dbscan_cluster'] == cluster]['cleaned_text'].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc77-k5Zq7Et",
        "outputId": "a18bae93-b1ab-49b7-c5d1-2e141ce04e0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/funnysahithi/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n",
            "                                             Reviews  \\\n",
            "0  I feel so LUCKY to have found this used (phone...   \n",
            "1  nice phone, nice up grade from my pantach revu...   \n",
            "2                                       Very pleased   \n",
            "3  It works good but it goes slow sometimes but i...   \n",
            "4  Great phone to replace my lost phone. The only...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  feel lucky found used phone us used hard phone...  \n",
            "1  nice phone nice grade pantach revue clean set ...  \n",
            "2                                            pleased  \n",
            "3     works good goes slow sometimes good phone love  \n",
            "4  great phone replace lost phone thing volume bu...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset (replace with your dataset path)\n",
        "file_path = '/Users/funnysahithi/Downloads/Amazon_Unlocked_Mobile.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Handle non-string values\n",
        "        return ''\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "data['cleaned_text'] = data['Reviews'].apply(preprocess_text)\n",
        "\n",
        "# Show cleaned data\n",
        "print(data[['Reviews', 'cleaned_text']].head())\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to avoid memory issues\n",
        "X = vectorizer.fit_transform(data['cleaned_text']).toarray()  # Convert sparse matrix to dense\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linkage_matrix = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linkage_matrix, truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "# Determine cluster labels\n",
        "num_clusters = 5  # Choose the number of desired clusters\n",
        "data['hierarchical_cluster'] = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "# Reduce dimensionality for visualization using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot the clustered data\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['hierarchical_cluster'], cmap='rainbow', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"Hierarchical Clustering Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze the clusters\n",
        "for i in range(1, num_clusters + 1):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(data[data['hierarchical_cluster'] == i]['cleaned_text'].head(10))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN4zA3Mfq7Et"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset (replace with your dataset path)\n",
        "file_path = '/Users/funnysahithi/Downloads/Amazon_Unlocked_Mobile.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Handle non-string values\n",
        "        return ''\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "data['cleaned_text'] = data['Reviews'].apply(preprocess_text)\n",
        "\n",
        "# Show cleaned data\n",
        "print(data[['Reviews', 'cleaned_text']].head())\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to avoid memory issues\n",
        "X = vectorizer.fit_transform(data['cleaned_text']).toarray()  # Convert sparse matrix to dense\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linkage_matrix = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "dendrogram(linkage_matrix, truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "# Determine cluster labels\n",
        "num_clusters = 5  # Choose the number of desired clusters\n",
        "data['hierarchical_cluster'] = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
        "\n",
        "# Reduce dimensionality for visualization using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot the clustered data\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['hierarchical_cluster'], cmap='rainbow', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"Hierarchical Clustering Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze the clusters\n",
        "for i in range(1, num_clusters + 1):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(data[data['hierarchical_cluster'] == i]['cleaned_text'].head(10))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhuPrIEGq7Eu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset (replace with your dataset path)\n",
        "file_path = '/Users/funnysahithi/Downloads/Amazon_Unlocked_Mobile.csv'  # Adjust the path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Handle non-string values\n",
        "        return ''\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the text column\n",
        "data['cleaned_text'] = data['Reviews'].fillna(\"\").apply(preprocess_text)\n",
        "\n",
        "# Show cleaned data\n",
        "print(\"\\nCleaned Data:\")\n",
        "print(data[['Reviews', 'cleaned_text']].head())\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# Function to generate BERT embeddings\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the mean of the last hidden state as the sentence embedding\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Generate BERT embeddings for all cleaned text\n",
        "data['bert_embedding'] = data['cleaned_text'].apply(lambda x: get_bert_embedding(x))\n",
        "\n",
        "# Convert embeddings to matrix form for clustering\n",
        "X = np.vstack(data['bert_embedding'])\n",
        "\n",
        "# ----------- K-means Clustering -----------\n",
        "num_clusters = 5  # Choose the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assign cluster labels to the data\n",
        "data['cluster'] = kmeans.labels_\n",
        "\n",
        "# Reduce dimensionality for visualization using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot the clustered data\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='rainbow', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"K-means Clustering with BERT Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze the clusters\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(data[data['cluster'] == i]['Reviews'].head(10))\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egySld49q7Eu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}