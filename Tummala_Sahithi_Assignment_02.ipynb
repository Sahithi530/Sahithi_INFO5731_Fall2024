{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahithi530/Sahithi_INFO5731_Fall2024/blob/main/Tummala_Sahithi_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jDyTKYs-yGit",
        "outputId": "ec247938-d45f-4c11-da1d-a04af88aa33b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1: 25 reviews collected.\n",
            "Page 2: 25 reviews collected.\n",
            "Page 3: 25 reviews collected.\n",
            "Page 4: 50 reviews collected.\n",
            "Page 5: 50 reviews collected.\n",
            "Page 6: 50 reviews collected.\n",
            "Page 7: 50 reviews collected.\n",
            "Page 8: 75 reviews collected.\n",
            "Page 9: 100 reviews collected.\n",
            "Page 10: 125 reviews collected.\n",
            "Page 11: 125 reviews collected.\n",
            "Page 12: 150 reviews collected.\n",
            "Page 13: 175 reviews collected.\n",
            "Page 14: 175 reviews collected.\n",
            "Page 15: 175 reviews collected.\n",
            "Page 16: 175 reviews collected.\n",
            "Page 17: 200 reviews collected.\n",
            "Page 18: 200 reviews collected.\n",
            "Page 19: 200 reviews collected.\n",
            "Page 20: 200 reviews collected.\n",
            "Page 21: 225 reviews collected.\n",
            "Page 22: 225 reviews collected.\n",
            "Page 23: 250 reviews collected.\n",
            "Page 24: 250 reviews collected.\n",
            "Page 25: 275 reviews collected.\n",
            "Page 26: 300 reviews collected.\n",
            "Page 27: 325 reviews collected.\n",
            "Page 28: 350 reviews collected.\n",
            "Page 29: 375 reviews collected.\n",
            "Page 30: 400 reviews collected.\n",
            "Page 31: 400 reviews collected.\n",
            "Page 32: 425 reviews collected.\n",
            "Page 33: 450 reviews collected.\n",
            "Page 34: 450 reviews collected.\n",
            "Page 35: 450 reviews collected.\n",
            "Page 36: 475 reviews collected.\n",
            "Page 37: 475 reviews collected.\n",
            "Page 38: 500 reviews collected.\n",
            "Page 39: 525 reviews collected.\n",
            "Page 40: 550 reviews collected.\n",
            "Page 41: 575 reviews collected.\n",
            "Page 42: 600 reviews collected.\n",
            "Page 43: 625 reviews collected.\n",
            "Page 44: 650 reviews collected.\n",
            "Page 45: 675 reviews collected.\n",
            "Page 46: 700 reviews collected.\n",
            "Page 47: 725 reviews collected.\n",
            "Page 48: 725 reviews collected.\n",
            "Page 49: 750 reviews collected.\n",
            "Page 50: 750 reviews collected.\n",
            "Page 51: 775 reviews collected.\n",
            "Page 52: 775 reviews collected.\n",
            "Page 53: 775 reviews collected.\n",
            "Page 54: 800 reviews collected.\n",
            "Page 55: 825 reviews collected.\n",
            "Page 56: 850 reviews collected.\n",
            "Page 57: 875 reviews collected.\n",
            "Page 58: 875 reviews collected.\n",
            "Page 59: 900 reviews collected.\n",
            "Page 60: 900 reviews collected.\n",
            "Page 61: 925 reviews collected.\n",
            "Page 62: 950 reviews collected.\n",
            "Page 63: 950 reviews collected.\n",
            "Page 64: 950 reviews collected.\n",
            "Page 65: 950 reviews collected.\n",
            "Page 66: 950 reviews collected.\n",
            "Page 67: 950 reviews collected.\n",
            "Page 68: 950 reviews collected.\n",
            "Page 69: 975 reviews collected.\n",
            "Page 70: 1000 reviews collected.\n",
            "Saved 1000 reviews to reviews.csv.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3f60224f-a4cd-41d1-af67-a7ac66917a7d\", \"reviews.csv\", 1297492)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Fetch reviews from IMDb\n",
        "def fetch_imdb_reviews(movie_id, max_reviews):\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "    while len(reviews) < max_reviews:\n",
        "        url = f\"https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3&start={page_number*10+1}&count=10\"\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='review-container')\n",
        "\n",
        "        for review_element in review_elements:\n",
        "            review_text_element = review_element.find('div', class_='text show-more__control')\n",
        "            if review_text_element is None:\n",
        "                continue\n",
        "            review_text = review_text_element.get_text(strip=True)\n",
        "            date_element = review_element.find('div', class_='display-name-date')\n",
        "            if date_element is None:\n",
        "                continue\n",
        "\n",
        "            date_span = date_element.find_all('span')\n",
        "            if not date_span or len(date_span) < 2:\n",
        "                continue\n",
        "            date_text = date_span[1].get_text(strip=True)\n",
        "            try:\n",
        "                review_date = datetime.strptime(date_text, \"%d %B %Y\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            if review_date.year == 2024:\n",
        "                reviews.append((review_text, review_date.strftime(\"%M-%d-%Y\")))\n",
        "\n",
        "            if len(reviews) >= max_reviews:\n",
        "                break\n",
        "\n",
        "        print(f\"Page {page_number}: {len(reviews)} reviews collected.\")\n",
        "        if len(reviews) >= max_reviews:\n",
        "            break\n",
        "\n",
        "        page_number += 1\n",
        "        time.sleep(2)\n",
        "\n",
        "    return reviews[:max_reviews]\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    kalki_movie_id = \"tt12735488\"\n",
        "    total_reviews_needed = 1000\n",
        "    reviews = fetch_imdb_reviews(kalki_movie_id, total_reviews_needed)\n",
        "    reviews = reviews[:total_reviews_needed]\n",
        "\n",
        "    # Save reviews to CSV\n",
        "    df = pd.DataFrame(reviews, columns=['Review', 'Date'])\n",
        "    df.to_csv('reviews.csv', index=False)\n",
        "    print(f\"Saved {len(reviews)} reviews to reviews.csv.\")\n",
        "\n",
        "    # Download the CSV\n",
        "    from google.colab import files\n",
        "    files.download('reviews.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "outputId": "64ae67b7-7f7b-4050-da77-1be1f95911aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to kalki_reviews_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans the text data by removing noise like special characters and punctuation.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "\n",
        "    # performs removal of special characters and punctuation\n",
        "    cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Loading the CSV file\n",
        "df = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Clean the 'Review' column by using the clean function and make a new 'Cleaned Review' column.\n",
        "df['Cleaned Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "#saving the new DataFrame to a new CSV file\n",
        "df.to_csv('kalki_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data saved to kalki_reviews_cleaned.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DRlP_sjBhlef",
        "outputId": "d803f76d-8c9f-4669-8596-243d4cedf2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Review', 'Date', 'Cleaned Review']\n",
            "Final cleaned data saved to kalki_reviews_final_cleaned.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_numbers(text):\n",
        "    \"\"\"Remove numbers from the text.\"\"\"\n",
        "    text = re.sub(r'\\d+', '', text)  # Removes all numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalizing whitespace\n",
        "    return text.strip()  # Removing leading and trailing whitespace\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "input_filename = 'kalki_reviews_cleaned.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns to verify\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# Check if 'Cleaned Review' exists in the DataFrame\n",
        "if 'Cleaned Review' in df.columns:\n",
        "    #removing numbers and creating a new column\n",
        "    df['Final_Cleaned_Review'] = df['Cleaned Review'].apply(remove_numbers)\n",
        "\n",
        "    # Save the final data to a new CSV file\n",
        "    output_filename = 'kalki_reviews_final_cleaned.csv'\n",
        "    df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"Final cleaned data saved to {output_filename}.\")\n",
        "else:\n",
        "    print(\"Column 'Cleaned Review' not found. Please check the column names.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "esBMaeaehleg",
        "outputId": "ece44859-05d7-408c-bdcc-152372f3b847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Review', 'Date', 'Cleaned Review', 'Final_Cleaned_Review']\n",
            "Cleaned Review column saved to cleaned_reviews_only.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#input the CSV file\n",
        "input_filename = 'kalki_reviews_final_cleaned.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# getting only 'Cleaned Review' column\n",
        "cleaned_review_df = df[['Cleaned Review']]  # Using double brackets to keep it as a DataFrame\n",
        "\n",
        "# Save data to a CSV file\n",
        "output_filename = 'cleaned_reviews_only.csv'\n",
        "cleaned_review_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Cleaned Review column saved to {output_filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZqTlXiqXhleg",
        "outputId": "7a262dce-086a-4474-ade9-7a0992db7ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Cleaned Review']\n",
            "Cleaned reviews without stopwords saved to kalki_reviews_no_stopwords.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample list of stopwords\n",
        "stopwords = set([\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "    'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
        "    'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',\n",
        "    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
        "    'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am',\n",
        "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
        "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
        "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
        "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
        "    'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
        "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
        "    'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
        "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
        "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
        "    'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don',\n",
        "    'should', 'now'\n",
        "])\n",
        "\n",
        "# input the csv\n",
        "input_filename = 'cleaned_reviews_only.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# Remove stopwords\n",
        "df['Reviews_No_Stopwords'] = df['Cleaned Review'].apply(\n",
        "    lambda text: ' '.join(word for word in text.split() if word.lower() not in stopwords)\n",
        ")\n",
        "\n",
        "# Save the data to a new CSV\n",
        "output_filename = 'kalki_reviews_no_stopwords.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Cleaned reviews without stopwords saved to {output_filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0CsWvNgKhleg",
        "outputId": "bb80e14c-5c6d-4120-df1c-24e9120e5104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Cleaned Review', 'Reviews_No_Stopwords']\n",
            "Lowercased reviews saved to kalki_reviews_lowercase.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# input CSV file containing reviews without stopwords\n",
        "input_filename = 'kalki_reviews_no_stopwords.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# Lowercase all text\n",
        "df['Lowercase_Review'] = df['Reviews_No_Stopwords'].str.lower()\n",
        "\n",
        "# Save the final data with lowercased\n",
        "output_filename = 'kalki_reviews_lowercase.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Lowercased reviews saved to {output_filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O0TCHNQfhleg",
        "outputId": "c0729470-8081-44e7-ec3a-7b2529d40e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Cleaned Review', 'Reviews_No_Stopwords', 'Lowercase_Review']\n",
            "Stemmed reviews saved to kalki_reviews_stemmed.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV file containing lowercase reviews\n",
        "input_filename = 'kalki_reviews_lowercase.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# verify\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# Initialize the Porter Stemmer to perform the fucntion\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to stem the words in the column\n",
        "def stem_review(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]  # Stem each word\n",
        "    return ' '.join(stemmed_words)  # Join the stemmed words back into a single string\n",
        "\n",
        "# Apply stemming to the 'Lowercase_Review' column\n",
        "df['Stemmed_Review'] = df['Lowercase_Review'].apply(stem_review)\n",
        "\n",
        "# Save the final data with stemmed reviews column name to a  file\n",
        "output_filename = 'kalki_reviews_stemmed.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Stemmed reviews saved to {output_filename}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P4u-mQ7uhleh",
        "outputId": "b162b69d-c342-46a1-b660-4e66bda1a621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Cleaned Review', 'Reviews_No_Stopwords', 'Lowercase_Review', 'Stemmed_Review']\n",
            "Lemmatized reviews saved to kalki_reviews_lemmatized.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV file containing lowercase reviews\n",
        "input_filename = 'kalki_reviews_stemmed.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns to verify\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize the words in the text\n",
        "def lemmatize_review(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
        "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
        "\n",
        "# Apply lemmatization to the 'Lowercase_Review' column\n",
        "df['Lemmatized_Review'] = df['Stemmed_Review'].apply(lemmatize_review)\n",
        "\n",
        "# Save the final data with lemmatized reviews to a new CSV file\n",
        "output_filename = 'kalki_reviews_lemmatized.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Lemmatized reviews saved to {output_filename}.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "outputId": "b4a5807d-ba07-4c64-b010-9f95ac9e9982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase_Review\n",
            "                                   Lemmatized_Review\n",
            "0  soar epic second part film excel climax taint ...\n",
            "1  replica star war movi supream leader look like...\n",
            "2  dont understand ob hero entri peopl good stori...\n",
            "3  didnt go big hope expect better adipurush way ...\n",
            "4  tricki justic big stori time limit regular mov...\n",
            "\n",
            "Total Counts from All Reviews:\n",
            "Nouns: 70120\n",
            "Verbs: 13400\n",
            "Adjectives: 25360\n",
            "Adverbs: 3920\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Download the averaged_perceptron_tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# loading the file and  reading only the specified column , I will be performing it to the clean text which is saved until lemmatized\n",
        "df = pd.read_csv('kalki_reviews_lemmatized.csv', usecols=['Lemmatized_Review'])\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"Lowercase_Review\")\n",
        "print(df.head())\n",
        "\n",
        "# Initialize the function\n",
        "def pos_analysis(text):\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    #POS\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Count occurrences of each part of speech\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "    # Define counts for Nouns, Verbs, Adjectives, and Adverbs\n",
        "    noun_count = sum(pos_counts.get(tag, 0) for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
        "    verb_count = sum(pos_counts.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
        "    adj_count = sum(pos_counts.get(tag, 0) for tag in ['JJ', 'JJR', 'JJS'])\n",
        "    adv_count = sum(pos_counts.get(tag, 0) for tag in ['RB', 'RBR', 'RBS'])\n",
        "\n",
        "    return noun_count, verb_count, adj_count, adv_count\n",
        "\n",
        "# Initialize counts\n",
        "total_nouns, total_verbs, total_adjectives, total_adverbs = 0, 0, 0, 0\n",
        "\n",
        "# Apply POS analysis to each lemmatized review in the specified column\n",
        "for review in df['Lemmatized_Review']:\n",
        "    nouns, verbs, adjectives, adverbs = pos_analysis(review)\n",
        "    total_nouns += nouns\n",
        "    total_verbs += verbs\n",
        "    total_adjectives += adjectives\n",
        "    total_adverbs += adverbs\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTotal Counts from All Reviews:\")\n",
        "print(f\"Nouns: {total_nouns}\")\n",
        "print(f\"Verbs: {total_verbs}\")\n",
        "print(f\"Adjectives: {total_adjectives}\")\n",
        "print(f\"Adverbs: {total_adverbs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0ZlfoHw3hleh",
        "outputId": "dd48d93f-cb9e-4a1e-f763-ec0fd59f0e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the DataFrame: Index(['Cleaned Review', 'Reviews_No_Stopwords', 'Lowercase_Review',\n",
            "       'Stemmed_Review', 'Lemmatized_Review'],\n",
            "      dtype='object')\n",
            "\n",
            "Parsing Review 1: soar epic second part film excel climax taint bloat bore technic terribl first half cring dialogu poorli choreograph action scene unnecessari subplotsth prop use throughout movi look like cheap foam much inspir drawn hollywood moviesth first attempt nag ashwin averag best hope improv significantli second part film focu mahabharata adapt faith possibl scifi element movi disappoint wherea charact great epic thoroughli exhilar\n",
            "Dependency Parsing Result:\n",
            "soar --> nmod --> film\n",
            "epic --> amod --> film\n",
            "second --> amod --> film\n",
            "part --> compound --> film\n",
            "film --> compound --> terribl\n",
            "excel --> compound --> terribl\n",
            "climax --> compound --> taint\n",
            "taint --> compound --> bloat\n",
            "bloat --> nmod --> terribl\n",
            "bore --> compound --> technic\n",
            "technic --> compound --> terribl\n",
            "terribl --> nsubj --> unnecessari\n",
            "first --> advmod --> cring\n",
            "half --> advmod --> cring\n",
            "cring --> acl --> terribl\n",
            "dialogu --> amod --> scene\n",
            "poorli --> amod --> scene\n",
            "choreograph --> compound --> action\n",
            "action --> compound --> scene\n",
            "scene --> dobj --> cring\n",
            "unnecessari --> ccomp --> charact\n",
            "subplotsth --> compound --> prop\n",
            "prop --> compound --> use\n",
            "use --> dobj --> unnecessari\n",
            "throughout --> prep --> unnecessari\n",
            "movi --> pobj --> throughout\n",
            "look --> dep --> unnecessari\n",
            "like --> prep --> look\n",
            "cheap --> amod --> inspir\n",
            "foam --> nmod --> inspir\n",
            "much --> amod --> inspir\n",
            "inspir --> pobj --> like\n",
            "drawn --> acl --> inspir\n",
            "hollywood --> compound --> moviesth\n",
            "moviesth --> dobj --> drawn\n",
            "first --> advmod --> attempt\n",
            "attempt --> conj --> unnecessari\n",
            "nag --> nmod --> hope\n",
            "ashwin --> compound --> hope\n",
            "averag --> compound --> hope\n",
            "best --> compound --> hope\n",
            "hope --> compound --> mahabharata\n",
            "improv --> compound --> mahabharata\n",
            "significantli --> nmod --> mahabharata\n",
            "second --> amod --> mahabharata\n",
            "part --> compound --> film\n",
            "film --> nmod --> mahabharata\n",
            "focu --> compound --> mahabharata\n",
            "mahabharata --> dobj --> attempt\n",
            "adapt --> xcomp --> unnecessari\n",
            "faith --> compound --> movi\n",
            "possibl --> compound --> movi\n",
            "scifi --> compound --> element\n",
            "element --> compound --> movi\n",
            "movi --> compound --> disappoint\n",
            "disappoint --> compound --> wherea\n",
            "wherea --> nsubj --> charact\n",
            "charact --> ROOT --> charact\n",
            "great --> amod --> thoroughli\n",
            "epic --> amod --> thoroughli\n",
            "thoroughli --> dobj --> charact\n",
            "exhilar --> advmod --> charact\n",
            "\n",
            "Constituency Parsing Result:\n",
            "(S\n",
            "  soar/VB\n",
            "  (NP\n",
            "    epic/JJ\n",
            "    second/JJ\n",
            "    part/NN\n",
            "    film/NN\n",
            "    excel/NN\n",
            "    climax/NN\n",
            "    taint/NN\n",
            "    bloat/NN\n",
            "    bore/NN)\n",
            "  (NP technic/JJ terribl/NN)\n",
            "  first/RB\n",
            "  (NP half/NN)\n",
            "  cring/VBG\n",
            "  (NP dialogu/JJ poorli/NN choreograph/NN action/NN scene/NN)\n",
            "  (NP unnecessari/JJ subplotsth/NN prop/NN use/NN)\n",
            "  (PP throughout/IN (NP movi/JJ look/NN))\n",
            "  (PP like/IN (NP cheap/JJ foam/NN))\n",
            "  (NP much/JJ inspir/NN drawn/NN hollywood/NN moviesth/NN)\n",
            "  (NP first/JJ attempt/NN nag/NN ashwin/NN averag/NN)\n",
            "  best/JJS\n",
            "  (NP hope/NN improv/NN)\n",
            "  (NP\n",
            "    significantli/JJ\n",
            "    second/JJ\n",
            "    part/NN\n",
            "    film/NN\n",
            "    focu/NN\n",
            "    mahabharata/NN\n",
            "    adapt/NN\n",
            "    faith/NN\n",
            "    possibl/NN)\n",
            "  (NP scifi/JJ element/NN movi/NN disappoint/NN wherea/NN charact/NN)\n",
            "  (NP great/JJ epic/NN thoroughli/NN exhilar/NN))\n",
            "Saved parsed reviews to 'kalki_reviews_parsed.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f7006483-41aa-4339-b870-7561e215205c\", \"kalki_reviews_parsed.csv\", 9589842)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "from google.colab import files\n",
        "\n",
        "# Loading the  model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Loading the updated version\n",
        "df = pd.read_csv('kalki_reviews_lemmatized.csv')  # Adjust the filename as necessary\n",
        "\n",
        "# Check the columns\n",
        "print(\"Columns in the DataFrame:\", df.columns)\n",
        "\n",
        "# Dependency Parsing\n",
        "def dependency_parse(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    parse_output = []\n",
        "    for token in doc:\n",
        "        parse_output.append(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
        "    return \"\\n\".join(parse_output)\n",
        "\n",
        "# Constituency Parsing\n",
        "def constituency_parse(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Defining simplication  for parsing\n",
        "    grammar = \"\"\"\n",
        "        NP: {<DT>?<JJ>*<NN.*>+}   # Noun Phrase\n",
        "        VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb Phrase\n",
        "        PP: {<IN><NP>}   # Prepositional Phrase\n",
        "        CLAUSE: {<NP><VP>}  # Clause\n",
        "    \"\"\"\n",
        "    cp = RegexpParser(grammar)\n",
        "    tree = cp.parse(tagged)\n",
        "\n",
        "    return str(tree)\n",
        "\n",
        "# Create new columns\n",
        "df['Dependency_Parse'] = \"\"\n",
        "df['Constituency_Parse'] = \"\"\n",
        "\n",
        "# going through each row\n",
        "for index, row in df.iterrows():\n",
        "    review = row['Lemmatized_Review']  # Use the actual column name here\n",
        "\n",
        "    #  parsing results\n",
        "    dep_parse_result = dependency_parse(review)\n",
        "    const_parse_result = constituency_parse(review)\n",
        "\n",
        "    # results are saved in dataframe\n",
        "    df.at[index, 'Dependency_Parse'] = dep_parse_result\n",
        "    df.at[index, 'Constituency_Parse'] = const_parse_result\n",
        "\n",
        "    # print one review for our understanding\n",
        "    if index == 0:\n",
        "        print(f\"\\nParsing Review 1: {review}\")\n",
        "        print(\"Dependency Parsing Result:\")\n",
        "        print(dep_parse_result)\n",
        "\n",
        "        print(\"\\nConstituency Parsing Result:\")\n",
        "        print(const_parse_result)\n",
        "\n",
        "# Save the data as a new column\n",
        "df.to_csv('kalki_reviews_parsed.csv', index=False)\n",
        "print(\"Saved parsed reviews to 'kalki_reviews_parsed.csv'.\")\n",
        "\n",
        "#downloading the csv file\n",
        "files.download('kalki_reviews_parsed.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "e4z7hj3shleh",
        "outputId": "ec7ef326-68a5-4084-921e-68a5f14432b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Cleaned Review', 'Reviews_No_Stopwords', 'Lowercase_Review', 'Stemmed_Review', 'Lemmatized_Review', 'Dependency_Parse', 'Constituency_Parse']\n",
            "\n",
            "Named Entity Counts:\n",
            "2898: 720\n",
            "sci fi movi: 120\n",
            "narr: 120\n",
            "max: 80\n",
            "storylin: 80\n",
            "everi: 80\n",
            "india: 80\n",
            "6000 year: 80\n",
            "kamal haasan: 80\n",
            "bachchan kamal hassan: 80\n",
            "replica star war movi: 40\n",
            "palpatin technolog: 40\n",
            "armi: 40\n",
            "betteri: 40\n",
            "london: 40\n",
            "first half: 40\n",
            "whenev prabha: 40\n",
            "happi wish: 40\n",
            "carri aswathamma: 40\n",
            "sharad kelkar hindi: 40\n",
            "1st half: 40\n",
            "2nd half: 40\n",
            "karna: 40\n",
            "uniqu genr: 40\n",
            "chao: 40\n",
            "brim potenti: 40\n",
            "heartkam hassan: 40\n",
            "trailerdisha patani: 40\n",
            "shobhana rajendra: 40\n",
            "narr entir movi: 40\n",
            "haasan despit: 40\n",
            "legendari: 40\n",
            "2015: 40\n",
            "sr ab: 40\n",
            "halfth: 40\n",
            "definit inspir: 40\n",
            "palac: 40\n",
            "buildup music: 40\n",
            "push boundari: 40\n",
            "hindi archaic style hindi: 40\n",
            "chemistri bhairava bujji: 40\n",
            "ashwatthama power bound plot: 40\n",
            "executionth start first half: 40\n",
            "abruptli chang: 40\n",
            "ashwatthama realli: 40\n",
            "inorgan cabley everyon: 40\n",
            "godli: 40\n",
            "ashwatthama theme: 40\n",
            "mayb nitpick: 40\n",
            "chang: 40\n",
            "inspir mahabharata: 40\n",
            "present day: 40\n",
            "extrem: 40\n",
            "howev: 40\n",
            "jack master: 40\n",
            "movi movi: 40\n",
            "nap: 40\n",
            "aswin: 40\n",
            "deliveri stun plenti surpris term star: 40\n",
            "castsir amitabh bachchan: 40\n",
            "natur: 40\n",
            "synopsi death punish blessingfirst: 40\n",
            "issu: 40\n",
            "experi howev: 40\n",
            "verdict kalki: 40\n",
            "visionari: 40\n",
            "extrem repetit prolong fight bhairava: 40\n",
            "kalki v kali karna: 40\n",
            "bachchan awesom: 40\n",
            "relat: 40\n",
            "sci: 40\n",
            "genr movi: 40\n",
            "bachchan kamal haasan: 40\n",
            "mahabharata ashwatthama: 40\n",
            "kaasi: 40\n",
            "includ sumati deepika: 40\n",
            "bachchan fulllength: 40\n",
            "indian mytholog: 40\n",
            "suprem leader: 40\n",
            "yaskin relat: 40\n",
            "scifi laudabl: 40\n",
            "last 30: 40\n",
            "minut 3hr: 40\n",
            "kasi v complex: 40\n",
            "narr slightli: 40\n",
            "terrif: 40\n",
            "kamal haasan suprem one: 40\n",
            "anna ben kyra: 40\n",
            "scifi movi hollywood: 40\n",
            "angel dune: 40\n",
            "unfortun: 40\n",
            "chao narr: 40\n",
            "innov: 40\n",
            "strongwil heroin: 40\n",
            "patani bring depth: 40\n",
            "genr: 40\n",
            "movi sleepi initi: 40\n",
            "publicis prabha: 40\n",
            "max furi: 40\n",
            "max indian matrix: 40\n",
            "mahabharata amitabh bachchan: 40\n",
            "weekend: 40\n",
            "710kalki 2898: 40\n",
            "kasi: 40\n",
            "suprem yaskin: 40\n",
            "war aveng: 40\n",
            "howev movi: 40\n",
            "thrill: 40\n",
            "experiencedespit initi: 40\n",
            "tamil malayalam kannada: 40\n",
            "nagi combin hindu mytholog: 40\n",
            "darl nagi smartli ad fan base: 40\n",
            "dulquer: 40\n",
            "extraordinari screenplayeveryon watch movi obscen: 40\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the cleaned CSV\n",
        "input_filename = 'kalki_reviews_parsed.csv'  # Adjust the path if needed\n",
        "df = pd.read_csv(input_filename)\n",
        "\n",
        "# Print the columns to verify\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "#  extract  entities and their counts\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        # Count specific entity types\n",
        "        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'DATE']:\n",
        "            entities[ent.text] = entities.get(ent.text, 0) + 1  # Simplified counting\n",
        "    return entities\n",
        "\n",
        "# Dictionary to hold all entities from all reviews\n",
        "all_entities = {}\n",
        "\n",
        "# Iterate through each review in the DataFrame, the lowercase review is our claned data , without being stemmed and lemmitizd\n",
        "for index, row in df.iterrows():\n",
        "    review = row['Lemmatized_Review']  # Ensure the correct column name\n",
        "    entities = extract_entities(review)\n",
        "\n",
        "    # Update the dictionary\n",
        "    for entity, count in entities.items():\n",
        "        all_entities[entity] = all_entities.get(entity, 0) + count  # Simplified counting\n",
        "\n",
        "# print the output\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "for entity, count in sorted(all_entities.items(), key=lambda item: item[1], reverse=True):\n",
        "    print(f\"{entity}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNn1lEVbMsv"
      },
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "outputs": [],
      "source": [
        "https://myunt-my.sharepoint.com/:x:/r/personal/sahithitummala_my_unt_edu/_layouts/15/Doc.aspx?sourcedoc=%7B7C82626C-FDC2-41F4-AD6F-C90888198205%7D&file=kalki_reviews_parsed%20(1).csv&action=default&mobileredirect=true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above link is for the csv file which consists of cleaned data without number, no stop words , lower case which is the actual cleaned data , the stemmed, the dependency parsing , constituency parsing"
      ],
      "metadata": {
        "id": "CTS6nWTawhwa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "outputs": [],
      "source": [
        "It was a whole level of new concept for me , but it helped me grab my concepts required to work further. The real challenge lied when i could see how the code varied for each set of cleaned data and how parsing was performed. This kind of challenge would definetly take a lot of time and effort"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}