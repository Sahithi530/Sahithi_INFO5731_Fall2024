{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahithi530/Sahithi_INFO5731_Fall2024/blob/main/Tummala_Sahithi_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Actually, prioritization encompasses one of the biggest areas in effective customer service management, which is already a very important duty in text classification.\n",
        "Of course, incoming emails can be categorized into different priority categories like Urgent, High, Medium, or Low automatically by the machine learning model itself.\n",
        "Some of the key features will be Readability Metrics, Sentiment Analysis, Topic Modeling, Lexical Aspects, Customer Metadata, Named Entity Recognition, and Historical Data Features.\n",
        "These lexical features encapsulate linguistic cues relevant to the urgency or importance of the issue while sentiment analysis aids in pre-ascertaining the overall sentiment score and the frequency of emotive terms.\n",
        "While topic modeling gives the main topics using techniques such as Latent Dirichlet Allocation, for example, structural features give background information about the type and history of a given problem.\n",
        "Examples of customer metadata include tier, account age, and past interaction history.\n",
        "\n",
        "Lexical capabilities are factors of a textual content's vocabulary that can be used to measure the pleasant of a written piece, in particular instructional writing. Lexical capabilities can consist of:\n",
        "Phrase duration: The period of words in a textual content\n",
        "Phrase frequency: How regularly phrases appear in a text\n",
        "Lexical density: The ratio of content phrases (nouns, verbs, adjectives, and adverbs) to the total variety of phrases in a text\n",
        "Lexical range: The ratio of different words to the total variety of words in a text\n",
        "Lexical sophistication: the proportion of excessive and coffee frequency phrases in a text\n",
        "Lexical features may be greater goal than precise words because they may be measured from the textual content's angle, instead of from a predefined lexicon. This objectivity can help keep away from biases and limitations associated with predefined word lists.\n",
        "A few not unusual lexical functions of instructional writing consist of:\n",
        "Precision and technicality: academic writing regularly makes use of unique and technical vocabulary\n",
        "Informational density: educational writing is regularly surprisingly based and incorporates a high percentage of nouns.\n",
        "\n",
        "\n",
        "In reality! Consider a textual content type venture wherein we're building a machine studying version to categorize film critiques as high-quality or terrible. This is a common sentiment evaluation assignment, and the dataset could consist of text reviews from movie-goers or critics. Here are diverse forms of features we could use for this task:\n",
        "\n",
        "1. Lexical capabilities:\n",
        "   a) word frequency: count of unique phrases or phrases.\n",
        "   B) N-grams: Sequences of 2-3 phrases that seem together.\n",
        "   C) TF-IDF (term Frequency-Inverse record Frequency): degree of word significance.\n",
        "   D) phrase period: average period of words used.\n",
        "\n",
        "Why helpful: those capabilities seize the vocabulary utilized in critiques. Tremendous critiques might use phrases like \"brilliant\" or \"pleasing\" greater frequently, even as negative reviews might have greater words like \"dull\" or \"disappointing\".\n",
        "\n",
        "2. Sentiment-based totally functions:\n",
        "   a) overall sentiment rating: using pre-educated sentiment analyzers.\n",
        "   B) Emotion phrases: Frequency of phrases related to exclusive feelings.\n",
        "   C) Polarity scores: measure of the way fine or negative certain terms are.\n",
        "\n",
        "Why beneficial: those immediately relate to the venture of figuring out whether a overview is positive or terrible. They capture the emotional tone of the text.\n",
        "\n",
        "3. Syntactic functions:\n",
        "   a) component-of-speech (POS) tags: Distribution of nouns, verbs, adjectives, and many others.\n",
        "   B) Dependency parsing: Grammatical shape of sentences.\n",
        "   C) Sentence period: average wide variety of words per sentence.\n",
        "\n",
        "Why useful: The manner sentences are constructed can indicate sentiment. For instance, more adjectives is probably utilized in expressive, emotional critiques.\n",
        "\n",
        "4. Stylometric features:\n",
        "   a) Punctuation utilization: Frequency of exclamation marks, question marks, etc.\n",
        "   B) Capitalization: Use of all-caps words (e.G., \"high-quality\" vs \"top notch\").\n",
        "   C) clarity rankings: Measures like Flesch-Kincaid readability index.\n",
        "\n",
        "Why useful: those capture writing fashion, which may be indicative of sentiment. Enthusiastic positive opinions may use extra exclamation factors, for example.\n",
        "\n",
        "5. Subject matter-based functions:\n",
        "   a) Latent Dirichlet Allocation (LDA) subjects: Distribution of summary \"topics\" within the text.\n",
        "   B) film-particular key phrases: Presence of phrases associated to plan, performing, computer graphics, etc.\n",
        "\n",
        "Why useful: unique elements of a movie (e.G., plot, appearing, visuals) is probably discussed more in tremendous or poor evaluations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_5Mq3MF0E6N",
        "outputId": "f292cb0d-f038-40b0-f236-644fc17c471c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (24.0)\n",
            "Collecting pip\n",
            "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.0\n",
            "    Uninstalling pip-24.0:\n",
            "      Successfully uninstalled pip-24.0\n",
            "Successfully installed pip-24.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOOZFcBS0E6O",
        "outputId": "6fd894bd-6b06-422a-dafe-8f968b7d5cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbTRCi7q0E6O",
        "outputId": "6b1bfd04-d112-406d-9065-ce86df3e1054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/funnysahithi/Library/Python/3.12/lib/python/site-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/funnysahithi/Library/Python/3.12/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKIG-29H0E6R",
        "outputId": "11f77f7f-3aa8-465a-c25e-8f0cfeec95ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) features:\n",
            " [[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0]\n",
            " [1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 2 0 0 1]]\n",
            "BoW feature names: ['address' 'an' 'beautiful' 'can' 'code' 'coffee' 'dress' 'drink' 'go'\n",
            " 'having' 'help' 'how' 'is' 'issues' 'me' 'need' 'tell' 'the' 'this' 'to'\n",
            " 'want' 'with' 'you']\n",
            "\n",
            "TF-IDF features:\n",
            " [[0.         0.         0.         0.         0.         0.52335825\n",
            "  0.         0.52335825 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.42224214 0.52335825 0.         0.        ]\n",
            " [0.38898761 0.48214012 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.48214012 0.\n",
            "  0.         0.         0.         0.48214012 0.         0.\n",
            "  0.         0.         0.         0.38898761 0.        ]\n",
            " [0.         0.         0.5        0.         0.         0.\n",
            "  0.5        0.         0.         0.         0.         0.\n",
            "  0.5        0.         0.         0.         0.         0.5\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.48214012 0.\n",
            "  0.         0.         0.         0.48214012 0.         0.\n",
            "  0.         0.48214012 0.         0.         0.         0.\n",
            "  0.38898761 0.         0.         0.38898761 0.        ]\n",
            " [0.25634472 0.         0.         0.31773267 0.         0.\n",
            "  0.         0.         0.31773267 0.         0.         0.31773267\n",
            "  0.         0.         0.31773267 0.         0.31773267 0.\n",
            "  0.25634472 0.51268944 0.         0.         0.31773267]]\n",
            "TF-IDF feature names: ['address' 'an' 'beautiful' 'can' 'code' 'coffee' 'dress' 'drink' 'go'\n",
            " 'having' 'help' 'how' 'is' 'issues' 'me' 'need' 'tell' 'the' 'this' 'to'\n",
            " 'want' 'with' 'you']\n",
            "\n",
            "N-gram features (Bigrams/Trigrams):\n",
            " [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0\n",
            "  0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 0 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0]\n",
            " [0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0\n",
            "  0 0 0 1 1]]\n",
            "N-gram feature names: ['an address' 'can you' 'can you tell' 'dress is' 'dress is beautiful'\n",
            " 'drink coffee' 'go to' 'go to this' 'having issues' 'having issues with'\n",
            " 'help with' 'help with an' 'how to' 'how to go' 'is beautiful'\n",
            " 'issues with' 'issues with this' 'me how' 'me how to' 'need help'\n",
            " 'need help with' 'tell me' 'tell me how' 'the dress' 'the dress is'\n",
            " 'this address' 'this code' 'to drink' 'to drink coffee' 'to go'\n",
            " 'to go to' 'to this' 'to this address' 'want to' 'want to drink'\n",
            " 'with an' 'with an address' 'with this' 'with this code' 'you tell'\n",
            " 'you tell me']\n",
            "\n",
            "Sentiment Scores:\n",
            "I want to drink a coffee. --> {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.0772}\n",
            "I need help with an address. --> {'neg': 0.0, 'neu': 0.597, 'pos': 0.403, 'compound': 0.4019}\n",
            "The dress is beautiful --> {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}\n",
            "I'm having issues with this code --> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Can you tell me how to go to this address? --> {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Word Embeddings (SpaCy):\n",
            "Text: I want to drink a coffee.\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.44839883 -0.38474578 -0.06534287 -0.28851113 -0.19042766]\n",
            "\n",
            "Text: I need help with an address.\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.12034736 -0.22475645  0.17446522 -0.24536903 -0.04142847]\n",
            "\n",
            "Text: The dress is beautiful\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [ 0.21935052  0.11040725 -0.12952143  0.5803616  -0.5538168 ]\n",
            "\n",
            "Text: I'm having issues with this code\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.3719103   0.17011206 -0.3418071   0.15209994  0.1703413 ]\n",
            "\n",
            "Text: Can you tell me how to go to this address?\n",
            "Embedding shape: (96,)\n",
            "First 5 elements of embedding: [-0.42561248 -0.42731324  0.29327983  0.03204915 -0.28894603]\n",
            "\n",
            "\n",
            "Length of Text (in words):\n",
            "I want to drink a coffee. --> 7 words\n",
            "I need help with an address. --> 7 words\n",
            "The dress is beautiful --> 4 words\n",
            "I'm having issues with this code --> 7 words\n",
            "Can you tell me how to go to this address? --> 11 words\n",
            "\n",
            "Part of Speech (POS) Tags:\n",
            "\n",
            "Text: I want to drink a coffee.\n",
            "I (PRON) want (VERB) to (PART) drink (VERB) a (DET) coffee (NOUN) . (PUNCT) \n",
            "\n",
            "Text: I need help with an address.\n",
            "I (PRON) need (VERB) help (NOUN) with (ADP) an (DET) address (NOUN) . (PUNCT) \n",
            "\n",
            "Text: The dress is beautiful\n",
            "The (DET) dress (NOUN) is (AUX) beautiful (ADJ) \n",
            "\n",
            "Text: I'm having issues with this code\n",
            "I (PRON) 'm (AUX) having (VERB) issues (NOUN) with (ADP) this (DET) code (NOUN) \n",
            "\n",
            "Text: Can you tell me how to go to this address?\n",
            "Can (AUX) you (PRON) tell (VERB) me (PRON) how (SCONJ) to (PART) go (VERB) to (ADP) this (DET) address (NOUN) ? (PUNCT) \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sample_texts = [\n",
        "    \"I want to drink a coffee.\",\n",
        "    \"I need help with an address.\",\n",
        "    \"The dress is beautiful\",\n",
        "    \"I'm having issues with this code\",\n",
        "    \"Can you tell me how to go to this address?\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(sample_texts)\n",
        "print(\"Bag of Words (BoW) features:\\n\", bow_features.toarray())\n",
        "print(\"BoW feature names:\", bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(sample_texts)\n",
        "print(\"\\nTF-IDF features:\\n\", tfidf_features.toarray())\n",
        "print(\"TF-IDF feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# N-grams\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
        "ngram_features = ngram_vectorizer.fit_transform(sample_texts)\n",
        "print(\"\\nN-gram features (Bigrams/Trigrams):\\n\", ngram_features.toarray())\n",
        "print(\"N-gram feature names:\", ngram_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Sentiment Analysis\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = [sentiment_analyzer.polarity_scores(text) for text in sample_texts]\n",
        "print(\"\\nSentiment Scores:\")\n",
        "for text, score in zip(sample_texts, sentiment_scores):\n",
        "    print(f\"{text} --> {score}\")\n",
        "\n",
        "# Word Embeddings\n",
        "print(\"\\nWord Embeddings (SpaCy):\")\n",
        "for text in sample_texts:\n",
        "    doc = nlp(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Embedding shape: {doc.vector.shape}\")\n",
        "    print(f\"First 5 elements of embedding: {doc.vector[:5]}\\n\")\n",
        "\n",
        "# Word Lengths\n",
        "text_lengths = [len(nltk.word_tokenize(text)) for text in sample_texts]\n",
        "print(\"\\nLength of Text (in words):\")\n",
        "for text, length in zip(sample_texts, text_lengths):\n",
        "    print(f\"{text} --> {length} words\")\n",
        "\n",
        "# POS Tags\n",
        "print(\"\\nPart of Speech (POS) Tags:\")\n",
        "for text in sample_texts:\n",
        "    doc = nlp(text)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} ({token.pos_})\", end=' ')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c2535f-f14d-4a47-f022-4fcf04ef058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Feature      Chi2\n",
            "0   delivery  0.500000\n",
            "1       late  0.500000\n",
            "2        the  0.500000\n",
            "3        was  0.500000\n",
            "4     devara  0.353553\n",
            "5   feedback  0.353553\n",
            "6      movie  0.353553\n",
            "7      named  0.353553\n",
            "8         on  0.353553\n",
            "9      share  0.353553\n",
            "10        to  0.353553\n",
            "11      want  0.353553\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#sampletext\n",
        "\n",
        "texts = [\n",
        "    \"I want to share a feedback on a movie named Devara\",\n",
        "    \"The delivery was late.\"\n",
        "]\n",
        "labels = [\"Feedback\", \"Complaint\"]\n",
        "\n",
        "#Create a TF-IDF\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#Converting data\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# chi-squared feature selection\n",
        "\n",
        "chi2_values, p_values = chi2(X, y)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2': chi2_values})\n",
        "chi2_df = chi2_df.sort_values(by='Chi2', ascending=False).reset_index(drop=True)\n",
        "\n",
        "#printthefeature\n",
        "\n",
        "print(chi2_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfaiurle0E6S",
        "outputId": "0870fe1a-5ebc-4ee1-a645-8aafca18bc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llvM8Wgt0E6S",
        "outputId": "7ff87b58-d3d1-4112-cea1-0739a7b7ac99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/funnysahithi/Library/Python/3.12/lib/python/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Using cached tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
            "Using cached transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "Using cached tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl (2.5 MB)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.25.1 tokenizers-0.20.0 transformers-4.45.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34609c04-22c9-4649-c1f2-ae225db5131e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: I am considering buying a new computer. Any guidelines?\n",
            "\n",
            "Ranked Texts based on Similarity:\n",
            "Text: I am considering buying a new computer. Any guidelines? -- Similarity Score: 1.0000\n",
            "Text: I'm having issues with the billing. -- Similarity Score: 0.7383\n",
            "Text: The customer support changed into terrible. I will by no means shop right here again. -- Similarity Score: 0.7074\n",
            "Text: I had the pleasant meal ever at that new eating place! Fantastically recommend it. -- Similarity Score: 0.6892\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#SAMPLE text\n",
        "sample_texts = [\n",
        "    \"I had the pleasant meal ever at that new eating place! Fantastically recommend it.\",\n",
        "    \"The customer support changed into terrible. I will by no means shop right here again.\",\n",
        "    \"I am considering buying a new computer. Any guidelines?\",\n",
        "    \"I'm having issues with the billing.\"\n",
        "]\n",
        "#BERT embedding for a text\n",
        "def get_bert_embedding(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "#BERT embedding for all the text samples\n",
        "\n",
        "text_embeddings = []\n",
        "for text in sample_texts:\n",
        "    text_embedding = get_bert_embedding(text, model, tokenizer)\n",
        "    text_embeddings.append(text_embedding)\n",
        "\n",
        "#concatinating\n",
        "\n",
        "text_embeddings = torch.cat(text_embeddings)\n",
        "\n",
        "query = \"I am considering buying a new computer. Any guidelines?\"\n",
        "\n",
        "query_embedding = get_bert_embedding(query, model, tokenizer)\n",
        "\n",
        "#sorting it in descending order\n",
        "\n",
        "cosine_similarities = cosine_similarity(query_embedding, text_embeddings)[0]\n",
        "\n",
        "ranked_indices = np.argsort(-cosine_similarities)\n",
        "\n",
        "#print the query and ranked texts\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"Ranked Texts based on Similarity:\")\n",
        "for idx in ranked_indices:\n",
        "    print(f\"Text: {sample_texts[idx]} -- Similarity Score: {cosine_similarities[idx]:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "Engaging inside the procedure of extracting functions from textual information furnished a precious getting to know opportunity. I discovered that expertise standards like tokenization, stemming, and lemmatization is vital for transforming text records for analysis. The punkt library performed a essential function in coaching me the way to effectively dissect text into sentences and phrases. I found that tokenization no longer just enables with later processing like extracting functions and classifying, however is also important for comprehending the text's shape and that means.\n",
        "\n",
        "Problems confronted\n",
        "\n",
        "One of the principal obstacles I confronted turned into making sure that the vital libraries and resources, just like the punkt tokenizer, had been nicely hooked up and to be had in my Python setup. This required fixing issues with paths and permissions, which may be annoying, mainly whilst the library appeared to reveal that it was cutting-edge. The demanding situations in gaining access to punkt underscored the significance of environmental management in NLP tasks.\n",
        "\n",
        "Significance for your vicinity of know-how\n",
        "\n",
        "This challenge may be very essential within the location of natural Language Processing (NLP) as characteristic extraction is a critical a part of getting textual statistics ready for device gaining knowledge of models. Knowing a way to tokenize and preprocess text is vital for greater complex NLP activities like sentiment evaluation, named entity reputation, and topic modeling. The problems encountered with the punkt library highlighted the importance of having a sturdy draw close of programming and linguistic concepts while working with NLP gear. In standard, this come across highlighted the vital hyperlink among textual content processing techniques and their implementation in realistic NLP assignments.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}